{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer_pt_utils.py:195: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  device: Optional[torch.device] = torch.device(\"cuda\"),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\T-Gamer\\\\Documents\\\\SideDrive\\\\UFMA\\\\2022.1\\\\Topicos Especiais (NLP)\\\\Exercicios\\\\Trabalho Final\\\\Implementação\\\\source'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DistilBertForSequenceClassification, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(source:str, target:str) -> int :\n",
    "    n = len(source)\n",
    "    m = len(target)\n",
    "    \n",
    "    D = np.zeros((n + 1, m + 1), dtype=int)\n",
    "    for i in range(1, n + 1) :\n",
    "        D[i][0] = D[i - 1][0] + 1\n",
    "    for j in range(1, m + 1) :\n",
    "        D[0][j] = D[0][j - 1] + 1\n",
    "\n",
    "    subst_cost = lambda x, y : 0 if x == y else 4\n",
    "    for i in range(1, n + 1) :\n",
    "        for j in range(1, m + 1) :\n",
    "            D[i][j] = min([D[i - 1][j    ] + 1,\n",
    "                           D[i - 1][j - 1] + subst_cost(source[i - 1], target[j - 1]),\n",
    "                           D[i    ][j - 1] + 1])\n",
    "    return D[n][m]\n",
    "def distance(source:str, target:str) -> float :\n",
    "    lev = levenshtein(source, target)\n",
    "    if source in target or target in source : lev = lev - .5 \n",
    "    return lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset) :\n",
    "    def __init__(self, \n",
    "                 dataframe:pd.DataFrame, \n",
    "                 vocabulario:list[str],\n",
    "                 max_length:int=512,\n",
    "                 text_column:str='text',\n",
    "                 label_column:str='class',\n",
    "                 n_classes:int=1) :\n",
    "        self.dataframe = dataframe\n",
    "        self.vocabulario = vocabulario\n",
    "        self.max_length = max_length\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.n_classes = n_classes\n",
    "    def __len__(self) :\n",
    "        return self.dataframe.shape[0]\n",
    "    def text_proxessing(self, text:str) :\n",
    "        tokenlist = text.split()\n",
    "        \n",
    "        sequence = []\n",
    "        for token in tokenlist :\n",
    "            # if not token in self.vocabulario :\n",
    "            #     voc_sort = self.vocabulario.copy()\n",
    "            #     voc_sort.sort(key=lambda x : distance(token, x))\n",
    "            #     token = voc_sort[0]\n",
    "            sequence.append(self.vocabulario.index(token) + 1)\n",
    "\n",
    "        mask = [1 for _ in range(len(sequence))]\n",
    "        if len(sequence) > self.max_length :\n",
    "            sequence = sequence[ : self.max_length]\n",
    "            mask = mask[ : self.max_length]\n",
    "        else :\n",
    "            padding = [0 for _ in range(self.max_length - len(sequence))]\n",
    "            sequence = sequence + padding\n",
    "            mask = mask + padding\n",
    "        \n",
    "        return sequence, mask\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "        # print(index, type(index))\n",
    "        sequence, mask = self.text_proxessing(self.dataframe.iloc[index][self.text_column])\n",
    "        label = self.dataframe.iloc[index][self.label_column]\n",
    "        if self.n_classes > 1 :\n",
    "            label = [(1 if i == int(label) else 0) for i in range(self.n_classes)]\n",
    "        else :\n",
    "            label = [label]\n",
    "        return {'input_ids'      : torch.tensor(sequence),\n",
    "                'attention_mask' : torch.tensor(mask),\n",
    "                'labels'         : torch.tensor(label)}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset:Dataset, embeddings:pd.DataFrame) :\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        num_labels=dataset.n_classes\n",
    "    )\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    embeddings_np_array = embeddings.values\n",
    "    embeddings_np_array = np.append(np.zeros((1, embeddings_np_array.shape[1])), embeddings_np_array, axis=0)\n",
    "    embeddings_module = torch.nn.Embedding.from_pretrained(\n",
    "        embeddings=torch.tensor(embeddings_np_array),\n",
    "        padding_idx=0\n",
    "    )\n",
    "    model.set_input_embeddings(embeddings_module)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='../resources/output',\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='../resources/logs',\n",
    "        logging_steps=10\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset\n",
    "    )\n",
    "    trainer.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"../resources/datasets/StanfordSentimentTreebank\"\n",
    "dataset_name   = \"SST2Processed-train\"\n",
    "embeddings_df = pd.read_csv(f\"{dataset_folder}/embeddings/{dataset_name}_dim768.csv\", index_col=0)\n",
    "dataset_df    = pd.read_csv(f\"{dataset_folder}/split/{dataset_name}.csv\",      index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8544, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(dataset_df, embeddings_df.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\T-Gamer/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\T-Gamer/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "c:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n"
     ]
    }
   ],
   "source": [
    "model = train(dataset, embeddings_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb071969cc081d156db4658a52cc12ff3302089485c0c8cb524fcf02c6362775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
