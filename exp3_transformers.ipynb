{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer_pt_utils.py:195: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  device: Optional[torch.device] = torch.device(\"cuda\"),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\T-Gamer\\\\Documents\\\\SideDrive\\\\UFMA\\\\2022.1\\\\Topicos Especiais (NLP)\\\\Exercicios\\\\Trabalho Final\\\\Implementação\\\\source'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DistilBertForSequenceClassification, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(source:str, target:str) -> int :\n",
    "    n = len(source)\n",
    "    m = len(target)\n",
    "    \n",
    "    D = np.zeros((n + 1, m + 1), dtype=int)\n",
    "    for i in range(1, n + 1) :\n",
    "        D[i][0] = D[i - 1][0] + 1\n",
    "    for j in range(1, m + 1) :\n",
    "        D[0][j] = D[0][j - 1] + 1\n",
    "\n",
    "    subst_cost = lambda x, y : 0 if x == y else 4\n",
    "    for i in range(1, n + 1) :\n",
    "        for j in range(1, m + 1) :\n",
    "            D[i][j] = min([D[i - 1][j    ] + 1,\n",
    "                           D[i - 1][j - 1] + subst_cost(source[i - 1], target[j - 1]),\n",
    "                           D[i    ][j - 1] + 1])\n",
    "    return D[n][m]\n",
    "def distance(source:str, target:str) -> float :\n",
    "    lev = levenshtein(source, target)\n",
    "    if source in target or target in source : lev = lev - .5 \n",
    "    return lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset) :\n",
    "    def __init__(self, \n",
    "                 dataframe:pd.DataFrame, \n",
    "                 vocabulario:list[str],\n",
    "                 max_length:int=512,\n",
    "                 text_column:str='text',\n",
    "                 label_column:str='class',\n",
    "                 n_classes:int=1) :\n",
    "        self.dataframe = dataframe\n",
    "        self.vocabulario = vocabulario\n",
    "        self.max_length = max_length\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.n_classes = n_classes\n",
    "    def __len__(self) :\n",
    "        return self.dataframe.shape[0]\n",
    "    def text_proxessing(self, text:str) :\n",
    "        tokenlist = text.split()\n",
    "        \n",
    "        sequence = []\n",
    "        for token in tokenlist :\n",
    "            # if not token in self.vocabulario :\n",
    "            #     voc_sort = self.vocabulario.copy()\n",
    "            #     voc_sort.sort(key=lambda x : distance(token, x))\n",
    "            #     token = voc_sort[0]\n",
    "            sequence.append(self.vocabulario.index(token) + 1)\n",
    "\n",
    "        mask = [1 for _ in range(len(sequence))]\n",
    "        if len(sequence) > self.max_length :\n",
    "            sequence = sequence[ : self.max_length]\n",
    "            mask = mask[ : self.max_length]\n",
    "        else :\n",
    "            padding = [0 for _ in range(self.max_length - len(sequence))]\n",
    "            sequence = sequence + padding\n",
    "            mask = mask + padding\n",
    "        \n",
    "        return sequence, mask\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "        # print(index, type(index))\n",
    "        sequence, mask = self.text_proxessing(self.dataframe.iloc[index][self.text_column])\n",
    "        label = self.dataframe.iloc[index][self.label_column]\n",
    "        if self.n_classes > 1 :\n",
    "            label = [int(label)]\n",
    "            # label = [([1] if i == int(label) else [0]) for i in range(self.n_classes)]\n",
    "        else :\n",
    "            label = [label]\n",
    "        label = torch.tensor(label)\n",
    "        return {'input_ids'      : torch.tensor(sequence),\n",
    "                'attention_mask' : torch.tensor(mask),\n",
    "                'labels'         : label}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset:Dataset, embeddings:pd.DataFrame, epochs:int, output_dir:str, logging_dir:str) -> DistilBertForSequenceClassification :\n",
    "    global resume_from_checkpoint\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        num_labels=dataset.n_classes\n",
    "    )\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    embeddings_np_array = embeddings.values\n",
    "    embeddings_np_array = np.append(np.zeros((1, embeddings_np_array.shape[1])), embeddings_np_array, axis=0)\n",
    "    embeddings_module = torch.nn.Embedding.from_pretrained(\n",
    "        embeddings=torch.tensor(embeddings_np_array),\n",
    "        padding_idx=0\n",
    "    )\n",
    "    model.set_input_embeddings(embeddings_module)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=logging_dir,\n",
    "        logging_steps=10\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset\n",
    "    )\n",
    "    trainer.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para dataset simples, sem folding\n",
    "# Args\n",
    "dataset_folder = \"../resources/datasets/StanfordSentimentTreebank\"\n",
    "dataset_name   = \"SST2Processed-train\"\n",
    "\n",
    "# Exec\n",
    "embeddings_df = pd.read_csv(f\"{dataset_folder}/embeddings/{dataset_name}.csv\", index_col=0)\n",
    "dataset_df    = pd.read_csv(f\"{dataset_folder}/split/{dataset_name}.csv\",      index_col=0)\n",
    "output_dir    = f\"../resources/output/{dataset_name}\"\n",
    "logging_dir   = f\"../resources/logs/{dataset_name}\"\n",
    "\n",
    "dataset = Dataset(dataset_df, embeddings_df.index.tolist())\n",
    "model = train(dataset, embeddings_df, epochs=5, output_dir=output_dir, logging_dir=logging_dir)\n",
    "model.save_pretrained(output_dir + \"/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 13176\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2472\n",
      "  0%|          | 10/2472 [04:38<19:25:58, 28.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1321, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/2472 [09:16<18:55:01, 27.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1136, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 30/2472 [13:47<18:23:53, 27.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0396, 'learning_rate': 3e-06, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 40/2472 [18:19<18:27:21, 27.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9914, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 50/2472 [22:54<18:17:21, 27.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9461, 'learning_rate': 5e-06, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 60/2472 [27:25<18:10:49, 27.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8986, 'learning_rate': 6e-06, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 70/2472 [31:56<18:01:29, 27.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8014, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 80/2472 [36:31<18:10:39, 27.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9407, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 90/2472 [41:04<18:20:51, 27.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7799, 'learning_rate': 9e-06, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 100/2472 [45:37<17:59:16, 27.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7643, 'learning_rate': 1e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 110/2472 [50:13<18:13:46, 27.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.77, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 120/2472 [54:46<17:47:20, 27.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7709, 'learning_rate': 1.2e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 130/2472 [59:19<17:41:21, 27.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7298, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 140/2472 [1:03:51<17:35:12, 27.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7445, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 150/2472 [1:08:22<17:26:49, 27.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6836, 'learning_rate': 1.5e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 160/2472 [1:12:55<17:21:10, 27.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8125, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 170/2472 [1:17:27<17:25:16, 27.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.771, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 180/2472 [1:21:58<17:10:23, 26.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7202, 'learning_rate': 1.8e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 190/2472 [1:26:28<17:10:00, 27.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8799, 'learning_rate': 1.9e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 200/2472 [1:31:00<17:04:51, 27.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7162, 'learning_rate': 2e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 210/2472 [1:35:30<17:01:52, 27.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7203, 'learning_rate': 2.1e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 220/2472 [1:40:01<16:57:37, 27.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7458, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 230/2472 [1:44:33<16:55:43, 27.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8245, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 240/2472 [1:49:06<16:58:06, 27.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7228, 'learning_rate': 2.4e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 250/2472 [1:53:39<16:53:09, 27.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6751, 'learning_rate': 2.5e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 260/2472 [1:58:15<16:48:18, 27.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7099, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 270/2472 [2:02:44<16:25:24, 26.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6674, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 280/2472 [2:07:14<16:24:48, 26.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8146, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 290/2472 [2:11:44<16:34:27, 27.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6554, 'learning_rate': 2.9e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 300/2472 [2:16:22<16:48:26, 27.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6093, 'learning_rate': 3e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 310/2472 [2:20:55<16:15:57, 27.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6759, 'learning_rate': 3.1e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 320/2472 [2:25:25<16:03:36, 26.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7037, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 330/2472 [2:29:55<16:01:33, 26.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8191, 'learning_rate': 3.3e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 340/2472 [2:34:24<15:55:35, 26.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6998, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 350/2472 [2:38:53<16:00:15, 27.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7815, 'learning_rate': 3.5e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 360/2472 [2:43:22<15:42:58, 26.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7191, 'learning_rate': 3.6e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 370/2472 [2:47:52<15:42:31, 26.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7509, 'learning_rate': 3.7e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 380/2472 [2:52:24<15:49:06, 27.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6984, 'learning_rate': 3.8e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 390/2472 [2:56:55<15:35:41, 26.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8204, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 400/2472 [3:01:23<15:25:15, 26.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7405, 'learning_rate': 4e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 410/2472 [3:05:51<15:17:08, 26.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7959, 'learning_rate': 4.1e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 420/2472 [3:10:19<15:18:52, 26.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6365, 'learning_rate': 4.2e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 430/2472 [3:14:49<15:15:26, 26.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7625, 'learning_rate': 4.3e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 440/2472 [3:19:16<14:59:34, 26.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7008, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 450/2472 [3:23:43<15:00:53, 26.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6058, 'learning_rate': 4.5e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 460/2472 [3:28:11<15:00:19, 26.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7783, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 470/2472 [3:32:40<14:56:02, 26.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7027, 'learning_rate': 4.7e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 480/2472 [3:37:09<14:56:19, 27.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6234, 'learning_rate': 4.8e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 490/2472 [3:41:40<14:56:23, 27.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7099, 'learning_rate': 4.9e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 500/2472 [3:46:09<14:39:47, 26.77s/it]Saving model checkpoint to ../resources/output/TweetsProcessed_Fold2\\checkpoint-500\n",
      "Configuration saved in ../resources/output/TweetsProcessed_Fold2\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5821, 'learning_rate': 5e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../resources/output/TweetsProcessed_Fold2\\checkpoint-500\\pytorch_model.bin\n",
      " 21%|██        | 510/2472 [3:50:41<14:40:16, 26.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6887, 'learning_rate': 4.974645030425964e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 520/2472 [3:55:11<14:39:19, 27.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6864, 'learning_rate': 4.949290060851927e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 530/2472 [3:59:41<14:28:12, 26.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7861, 'learning_rate': 4.923935091277891e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 540/2472 [4:04:10<14:25:48, 26.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7535, 'learning_rate': 4.898580121703854e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 550/2472 [4:08:43<14:36:12, 27.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8945, 'learning_rate': 4.873225152129818e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 560/2472 [4:13:13<14:18:33, 26.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6996, 'learning_rate': 4.847870182555781e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 570/2472 [4:17:41<14:08:11, 26.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.672, 'learning_rate': 4.8225152129817444e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 580/2472 [4:22:13<14:23:15, 27.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7025, 'learning_rate': 4.7971602434077076e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 590/2472 [4:26:42<14:05:02, 26.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7745, 'learning_rate': 4.7718052738336714e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 600/2472 [4:31:10<13:55:03, 26.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6667, 'learning_rate': 4.746450304259635e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 610/2472 [4:35:37<13:42:37, 26.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6978, 'learning_rate': 4.7210953346855984e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 620/2472 [4:40:04<13:45:49, 26.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7471, 'learning_rate': 4.695740365111562e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 630/2472 [4:44:32<13:42:21, 26.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6332, 'learning_rate': 4.6703853955375254e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 640/2472 [4:49:02<13:49:19, 27.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.663, 'learning_rate': 4.645030425963489e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 650/2472 [4:53:32<13:35:25, 26.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7098, 'learning_rate': 4.6196754563894524e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 660/2472 [4:58:02<13:32:05, 26.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6688, 'learning_rate': 4.594320486815416e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 670/2472 [5:02:29<13:17:15, 26.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6841, 'learning_rate': 4.5689655172413794e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 680/2472 [5:06:56<13:21:29, 26.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.746, 'learning_rate': 4.543610547667343e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 690/2472 [5:11:26<13:25:17, 27.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.722, 'learning_rate': 4.5182555780933065e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 700/2472 [5:15:55<13:11:28, 26.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7133, 'learning_rate': 4.4929006085192696e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 710/2472 [5:20:25<13:17:12, 27.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6223, 'learning_rate': 4.4675456389452335e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 720/2472 [5:24:54<13:03:36, 26.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9443, 'learning_rate': 4.4421906693711966e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 730/2472 [5:29:22<12:53:16, 26.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7526, 'learning_rate': 4.4168356997971605e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 740/2472 [5:33:52<12:58:33, 26.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6993, 'learning_rate': 4.3914807302231236e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 750/2472 [5:38:20<12:47:22, 26.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7198, 'learning_rate': 4.3661257606490875e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 760/2472 [5:42:50<12:49:29, 26.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5971, 'learning_rate': 4.340770791075051e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 770/2472 [5:47:16<12:35:51, 26.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6404, 'learning_rate': 4.3154158215010145e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 780/2472 [5:51:42<12:31:34, 26.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6117, 'learning_rate': 4.290060851926978e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 790/2472 [5:56:10<12:34:04, 26.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7753, 'learning_rate': 4.2647058823529415e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 800/2472 [6:00:37<12:18:14, 26.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7054, 'learning_rate': 4.2393509127789046e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 810/2472 [6:05:04<12:18:30, 26.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7028, 'learning_rate': 4.213995943204868e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 812/2472 [6:05:58<12:23:25, 26.87s/it]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'nan' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\T-Gamer\\Documents\\SideDrive\\UFMA\\2022.1\\Topicos Especiais (NLP)\\Exercicios\\Trabalho Final\\Implementação\\source\\exp3_transformers.ipynb Célula: 6\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         os\u001b[39m.\u001b[39mmakedirs(\u001b[39mdir\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m dataset \u001b[39m=\u001b[39m Dataset(dataset_df, embeddings_df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mtolist(), label_column\u001b[39m=\u001b[39mlabel_column, n_classes\u001b[39m=\u001b[39mn_classes)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m model \u001b[39m=\u001b[39m train(dataset, embeddings_df, epochs\u001b[39m=\u001b[39;49mn_epochs, output_dir\u001b[39m=\u001b[39;49moutput_dir, logging_dir\u001b[39m=\u001b[39;49mlogging_dir)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(output_dir \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/final\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\T-Gamer\\Documents\\SideDrive\\UFMA\\2022.1\\Topicos Especiais (NLP)\\Exercicios\\Trabalho Final\\Implementação\\source\\exp3_transformers.ipynb Célula: 6\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, embeddings, epochs, output_dir, logging_dir)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     output_dir\u001b[39m=\u001b[39moutput_dir,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39mepochs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     logging_steps\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mdataset\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1498\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1495\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1497\u001b[0m )\n\u001b[1;32m-> 1498\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1499\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1500\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1501\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1502\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1503\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1714\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[0;32m   1713\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1714\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1715\u001b[0m \n\u001b[0;32m   1716\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[0;32m   1717\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1718\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\T-Gamer\\Documents\\SideDrive\\UFMA\\2022.1\\Topicos Especiais (NLP)\\Exercicios\\Trabalho Final\\Implementação\\source\\exp3_transformers.ipynb Célula: 6\u001b[0m in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index) :\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# print(index, type(index))\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     sequence, mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_proxessing(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataframe\u001b[39m.\u001b[39;49miloc[index][\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_column])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataframe\u001b[39m.\u001b[39miloc[index][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_column]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m :\n",
      "\u001b[1;32mc:\\Users\\T-Gamer\\Documents\\SideDrive\\UFMA\\2022.1\\Topicos Especiais (NLP)\\Exercicios\\Trabalho Final\\Implementação\\source\\exp3_transformers.ipynb Célula: 6\u001b[0m in \u001b[0;36mDataset.text_proxessing\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m sequence \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokenlist :\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# if not token in self.vocabulario :\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m#     voc_sort = self.vocabulario.copy()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m#     voc_sort.sort(key=lambda x : distance(token, x))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m#     token = voc_sort[0]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     sequence\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocabulario\u001b[39m.\u001b[39;49mindex(token) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m mask \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(sequence))]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(sequence) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length :\n",
      "\u001b[1;31mValueError\u001b[0m: 'nan' is not in list"
     ]
    }
   ],
   "source": [
    "# Para dataset com k-folding\n",
    "# Args\n",
    "k = 10\n",
    "dataset_folder = \"../resources/datasets/TwitterAirlines\"\n",
    "dataset_name   = \"TweetsProcessed\"\n",
    "n_classes = 3\n",
    "n_epochs = 3\n",
    "label_column = \"class\"\n",
    "# Exec\n",
    "folds = []\n",
    "for i_fold in range(k) :\n",
    "    dataset_fold_name = f\"{dataset_name}_Fold{i_fold + 1}\"\n",
    "    fold_df = pd.read_csv(f\"{dataset_folder}/folds/{dataset_fold_name}.csv\", index_col = 0)\n",
    "    folds.append(fold_df)\n",
    "for i_fold in range(1, k) :\n",
    "    dataset_fold_name = f\"{dataset_name}_Fold{i_fold + 1}\"\n",
    "    embeddings_df = pd.read_csv(f\"{dataset_folder}/embeddings/{dataset_fold_name}.csv\", index_col=0)\n",
    "    dataset_df    = pd.concat(folds[ : i_fold] + folds[i_fold + 1 : ])\n",
    "    \n",
    "    output_dir    = f\"../resources/output/{dataset_fold_name}\"\n",
    "    logging_dir   = f\"../resources/logs/{dataset_fold_name}\"\n",
    "    for dir in (output_dir, logging_dir) :\n",
    "        if not os.path.exists(dir) : \n",
    "            os.makedirs(dir)\n",
    "\n",
    "    dataset = Dataset(dataset_df, embeddings_df.index.tolist(), label_column=label_column, n_classes=n_classes)\n",
    "    model = train(dataset, embeddings_df, epochs=n_epochs, output_dir=output_dir, logging_dir=logging_dir)\n",
    "    model.save_pretrained(output_dir + \"/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb071969cc081d156db4658a52cc12ff3302089485c0c8cb524fcf02c6362775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
