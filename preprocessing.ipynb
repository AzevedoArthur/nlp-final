{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\T-Gamer\\\\Documents\\\\SideDrive\\\\UFMA\\\\2022.1\\\\Topicos Especiais (NLP)\\\\Exercicios\\\\Trabalho Final\\\\Implementação\\\\source'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ats_hashtags_and_links(text:str) -> str :\n",
    "    gross_tokenlist = text.split()\n",
    "    text = \"\"\n",
    "    for gross_token in gross_tokenlist :\n",
    "        if not (gross_token.startswith(\"@\") or\n",
    "                gross_token.startswith(\"#\") or\n",
    "                gross_token.startswith(\"http\")) :\n",
    "            text += gross_token + \" \"\n",
    "    return text[:-1]\n",
    "def remove_stopwords(tokenlist:list[str]) -> list[str] :\n",
    "    stoplist = stopwords.words('english')\n",
    "    newtokenlist = []\n",
    "    for token in tokenlist :\n",
    "        if not token in stoplist :\n",
    "            newtokenlist.append(token)\n",
    "    return newtokenlist\n",
    "def remove_nonalpha(tokenlist:list[str]) -> list[str] :\n",
    "    newtokenlist = []\n",
    "    for tkn in tokenlist :\n",
    "        newtkn = \"\"\n",
    "        for char in tkn :\n",
    "            if char.isalpha() :\n",
    "                newtkn += char\n",
    "        if newtkn != \"\" : newtokenlist.append(newtkn)\n",
    "    return newtokenlist\n",
    "def preprocess(text:str) -> str :\n",
    "    text = remove_ats_hashtags_and_links(text)\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    tokenlist = nltk.word_tokenize(text)\n",
    "    tokenlist = remove_stopwords(tokenlist)\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lemmalist = [lemmatizer.lemmatize(token) for token in tokenlist]\n",
    "    lemmalist = remove_nonalpha(lemmalist)\n",
    "    text = \"\"\n",
    "    for tkn in lemmalist :\n",
    "        text += tkn + \" \"\n",
    "    text = text[ : -1]\n",
    "    if text == \"\" : return \" \" # Treat NaN\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twiter Airlines\n",
    "dataset_folder = \"../resources/datasets/TwitterAirlines\"\n",
    "\n",
    "dataset = pd.read_csv(f\"{dataset_folder}/Tweets.csv\", index_col=0)\n",
    "\n",
    "columns_to_keep = [\"airline_sentiment\", \"airline_sentiment_confidence\", \"text\"]\n",
    "dataset_clean = dataset.iloc[ : , [column in columns_to_keep for column in dataset.columns]]\n",
    "dataset_clean.to_csv(f\"{dataset_folder}/TweetsDroppedCols.csv\")\n",
    "\n",
    "dataset_clean = dataset_clean.copy() # Defrag\n",
    "\n",
    "processed_text = []\n",
    "classes = []\n",
    "index = []\n",
    "for i, (_, row) in enumerate(dataset_clean.iterrows()) :\n",
    "    if i % 100 == 0 : \n",
    "        print(i)\n",
    "    processed = preprocess(row[\"text\"])\n",
    "    if processed == \"\" : processed = \" \"\n",
    "    processed_text.append(processed)\n",
    "    classes.append([\"negative\", \"neutral\", \"positive\"].index(row[\"airline_sentiment\"]))\n",
    "    index.append(row.name)\n",
    "processed_text_column = pd.Series(processed_text, index, name=\"text\")\n",
    "classes_column = pd.Series(classes, index, name=\"class\")\n",
    "\n",
    "dataset_processed = pd.concat([processed_text_column, classes_column], axis=1)\n",
    "dataset_processed.to_csv(f\"{dataset_folder}/TweetsProcessed.csv\")\n",
    "dataset_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>splitset</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rock destined st century s new conan s going m...</td>\n",
       "      <td>226166</td>\n",
       "      <td>1</td>\n",
       "      <td>0.69444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gorgeously elaborate continuation lord ring tr...</td>\n",
       "      <td>226300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.83333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>effective tepid biopic</td>\n",
       "      <td>13995</td>\n",
       "      <td>2</td>\n",
       "      <td>0.51389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sometimes like go movie fun wasabi good place ...</td>\n",
       "      <td>14123</td>\n",
       "      <td>2</td>\n",
       "      <td>0.73611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>emerges something rare issue movie s honest ke...</td>\n",
       "      <td>13999</td>\n",
       "      <td>2</td>\n",
       "      <td>0.86111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11851</th>\n",
       "      <td>real snooze</td>\n",
       "      <td>222071</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11852</th>\n",
       "      <td>surprise</td>\n",
       "      <td>225165</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11853</th>\n",
       "      <td>ve seen hippie turned yuppie plot s enthusiast...</td>\n",
       "      <td>226985</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11854</th>\n",
       "      <td>fan walked muttering word like horrible terrib...</td>\n",
       "      <td>223632</td>\n",
       "      <td>1</td>\n",
       "      <td>0.13889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11855</th>\n",
       "      <td>case zero</td>\n",
       "      <td>224044</td>\n",
       "      <td>1</td>\n",
       "      <td>0.34722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11855 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  phrase_id  splitset  \\\n",
       "1      rock destined st century s new conan s going m...     226166         1   \n",
       "2      gorgeously elaborate continuation lord ring tr...     226300         1   \n",
       "3                                 effective tepid biopic      13995         2   \n",
       "4      sometimes like go movie fun wasabi good place ...      14123         2   \n",
       "5      emerges something rare issue movie s honest ke...      13999         2   \n",
       "...                                                  ...        ...       ...   \n",
       "11851                                        real snooze     222071         1   \n",
       "11852                                           surprise     225165         1   \n",
       "11853  ve seen hippie turned yuppie plot s enthusiast...     226985         1   \n",
       "11854  fan walked muttering word like horrible terrib...     223632         1   \n",
       "11855                                          case zero     224044         1   \n",
       "\n",
       "         class  \n",
       "1      0.69444  \n",
       "2      0.83333  \n",
       "3      0.51389  \n",
       "4      0.73611  \n",
       "5      0.86111  \n",
       "...        ...  \n",
       "11851  0.11111  \n",
       "11852  0.22222  \n",
       "11853  0.75000  \n",
       "11854  0.13889  \n",
       "11855  0.34722  \n",
       "\n",
       "[11855 rows x 4 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stanford Sentiment Treebank 2\n",
    "dataset_folder = \"../resources/datasets/StanfordSentimentTreebank\"\n",
    "\n",
    "dataset = pd.read_csv(f\"{dataset_folder}/SST2.csv\", index_col=0)\n",
    "\n",
    "processed_text = []\n",
    "index = []\n",
    "for i, (_, row) in enumerate(dataset.iterrows()) :\n",
    "    if i % 100 == 0 : \n",
    "        print(i)\n",
    "    processed = preprocess(row[\"text\"])\n",
    "    if processed == \"\" : processed = \" \"\n",
    "    processed_text.append(processed)\n",
    "    index.append(row.name)\n",
    "processed_text_column = pd.Series(processed_text, index, name=\"text\")\n",
    "\n",
    "dataset_processed = dataset.copy()\n",
    "dataset_processed[\"text\"] = processed_text_column\n",
    "dataset_processed.to_csv(f\"{dataset_folder}/SST2Processed.csv\")\n",
    "dataset_processed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb071969cc081d156db4658a52cc12ff3302089485c0c8cb524fcf02c6362775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
