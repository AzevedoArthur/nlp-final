{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\T-Gamer\\\\Documents\\\\SideDrive\\\\UFMA\\\\2022.1\\\\Topicos Especiais (NLP)\\\\Exercicios\\\\Trabalho Final\\\\Implementação\\\\source'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ats_hashtags_and_links(text:str) -> str :\n",
    "    gross_tokenlist = text.split()\n",
    "    text = \"\"\n",
    "    for gross_token in gross_tokenlist :\n",
    "        if not (gross_token.startswith(\"@\") or\n",
    "                gross_token.startswith(\"#\") or\n",
    "                gross_token.startswith(\"http\")) :\n",
    "            text += gross_token + \" \"\n",
    "    return text[:-1]\n",
    "def remove_stopwords(tokenlist:list[str]) -> list[str] :\n",
    "    stoplist = stopwords.words('english')\n",
    "    newtokenlist = []\n",
    "    for token in tokenlist :\n",
    "        if not token in stoplist :\n",
    "            newtokenlist.append(token)\n",
    "    return newtokenlist\n",
    "def remove_nonalpha(tokenlist:list[str]) -> list[str] :\n",
    "    newtokenlist = []\n",
    "    for tkn in tokenlist :\n",
    "        newtkn = \"\"\n",
    "        for char in tkn :\n",
    "            if char.isalpha() :\n",
    "                newtkn += char\n",
    "        if newtkn != \"\" : newtokenlist.append(newtkn)\n",
    "    return newtokenlist\n",
    "def preprocess(text:str) -> str :\n",
    "    text = remove_ats_hashtags_and_links(text)\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    tokenlist = nltk.word_tokenize(text)\n",
    "    tokenlist = remove_stopwords(tokenlist)\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lemmalist = [lemmatizer.lemmatize(token) for token in tokenlist]\n",
    "    lemmalist = remove_nonalpha(lemmalist)\n",
    "    text = \"\"\n",
    "    for tkn in lemmalist :\n",
    "        text += tkn + \" \"\n",
    "    text = text[ : -1]\n",
    "    if text == \"\" : return \" \" # Treat NaN\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twiter Airlines\n",
    "dataset_folder = \"../resources/datasets/TwitterAirlines\"\n",
    "\n",
    "dataset = pd.read_csv(f\"{dataset_folder}/Tweets.csv\", index_col=0)\n",
    "\n",
    "columns_to_keep = [\"airline_sentiment\", \"airline_sentiment_confidence\", \"text\"]\n",
    "dataset_clean = dataset.iloc[ : , [column in columns_to_keep for column in dataset.columns]]\n",
    "dataset_clean.to_csv(f\"{dataset_folder}/TweetsDroppedCols.csv\")\n",
    "\n",
    "dataset_clean = dataset_clean.copy() # Defrag\n",
    "\n",
    "processed_text = []\n",
    "classes = []\n",
    "index = []\n",
    "for i, (_, row) in enumerate(dataset_clean.iterrows()) :\n",
    "    if i % 100 == 0 : \n",
    "        print(i)\n",
    "    processed = preprocess(row[\"text\"])\n",
    "    if processed == \"\" : processed = \" \"\n",
    "    processed_text.append(processed)\n",
    "    classes.append([\"negative\", \"neutral\", \"positive\"].index(row[\"airline_sentiment\"]))\n",
    "    index.append(row.name)\n",
    "processed_text_column = pd.Series(processed_text, index, name=\"text\")\n",
    "classes_column = pd.Series(classes, index, name=\"class\")\n",
    "\n",
    "dataset_processed = pd.concat([processed_text_column, classes_column], axis=1)\n",
    "dataset_processed.to_csv(f\"{dataset_folder}/TweetsProcessed.csv\")\n",
    "dataset_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5814_8</th>\n",
       "      <td>1</td>\n",
       "      <td>stuff going moment mj ve started listening mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381_9</th>\n",
       "      <td>1</td>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7759_3</th>\n",
       "      <td>0</td>\n",
       "      <td>film start manager nicholas bell giving welcom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3630_4</th>\n",
       "      <td>0</td>\n",
       "      <td>must assumed praised film greatest filmed oper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9495_8</th>\n",
       "      <td>1</td>\n",
       "      <td>superbly trashy wondrously unpretentious s exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3453_3</th>\n",
       "      <td>0</td>\n",
       "      <td>seems like consideration gone imdb review film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5064_1</th>\n",
       "      <td>0</td>\n",
       "      <td>nt believe made film completely unnecessary fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10905_3</th>\n",
       "      <td>0</td>\n",
       "      <td>guy loser ca nt get girl need build picked str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10194_3</th>\n",
       "      <td>0</td>\n",
       "      <td>minute documentary buñuel made early s one spa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8478_8</th>\n",
       "      <td>1</td>\n",
       "      <td>saw movie child broke heart story unfinished e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                               text\n",
       "5814_8       1  stuff going moment mj ve started listening mus...\n",
       "2381_9       1  classic war worlds timothy hines entertaining ...\n",
       "7759_3       0  film start manager nicholas bell giving welcom...\n",
       "3630_4       0  must assumed praised film greatest filmed oper...\n",
       "9495_8       1  superbly trashy wondrously unpretentious s exp...\n",
       "...        ...                                                ...\n",
       "3453_3       0  seems like consideration gone imdb review film...\n",
       "5064_1       0  nt believe made film completely unnecessary fi...\n",
       "10905_3      0  guy loser ca nt get girl need build picked str...\n",
       "10194_3      0  minute documentary buñuel made early s one spa...\n",
       "8478_8       1  saw movie child broke heart story unfinished e...\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stanford Sentiment Treebank 2 e IMDB Movie Reviews\n",
    "# Args\n",
    "dataset_folder = \"../resources/datasets/IMDBMovieReviews\"\n",
    "dataset_name = \"IMDB_MR\"\n",
    "# dataset_folder = \"../resources/datasets/StanfordSentimentTreebank\"\n",
    "# dataset_name = \"SST2\"\n",
    "# Exec\n",
    "dataset = pd.read_csv(f\"{dataset_folder}/{dataset_name}.csv\", index_col=0)\n",
    "\n",
    "processed_text = []\n",
    "index = []\n",
    "for i, (_, row) in enumerate(dataset.iterrows()) :\n",
    "    if i % 100 == 0 : \n",
    "        print(i)\n",
    "    processed = preprocess(row[\"text\"])\n",
    "    if processed == \"\" : processed = \" \"\n",
    "    processed_text.append(processed)\n",
    "    index.append(row.name)\n",
    "processed_text_column = pd.Series(processed_text, index, name=\"text\")\n",
    "\n",
    "dataset_processed = dataset.copy()\n",
    "dataset_processed[\"text\"] = processed_text_column\n",
    "dataset_processed.to_csv(f\"{dataset_folder}/{dataset_name}Processed.csv\")\n",
    "dataset_processed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb071969cc081d156db4658a52cc12ff3302089485c0c8cb524fcf02c6362775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
