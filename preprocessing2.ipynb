{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\T-Gamer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\T-Gamer\\\\Documents\\\\SideDrive\\\\UFMA\\\\2022.1\\\\Topicos Especiais (NLP)\\\\Exercicios\\\\Trabalho Final\\\\Implementação\\\\source'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ats_hashtags_and_links(text:str) -> str :\n",
    "    gross_tokenlist = text.split()\n",
    "    text = \"\"\n",
    "    for gross_token in gross_tokenlist :\n",
    "        if not (gross_token.startswith(\"@\") or\n",
    "                gross_token.startswith(\"#\") or\n",
    "                gross_token.startswith(\"http\")) :\n",
    "            text += gross_token + \" \"\n",
    "    return text[:-1]\n",
    "def remove_stopwords(tokenlist:list[str]) -> list[str] :\n",
    "    stoplist = stopwords.words('english')\n",
    "    newtokenlist = []\n",
    "    for token in tokenlist :\n",
    "        if not token in stoplist :\n",
    "            newtokenlist.append(token)\n",
    "    return newtokenlist\n",
    "def remove_nonalpha(tokenlist:list[str]) -> list[str] :\n",
    "    newtokenlist = []\n",
    "    for tkn in tokenlist :\n",
    "        newtkn = \"\"\n",
    "        for char in tkn :\n",
    "            if char.isalnum() :\n",
    "                newtkn += char\n",
    "        if newtkn != \"\" : newtokenlist.append(newtkn)\n",
    "    return newtokenlist\n",
    "def preprocess(text:str) -> str :\n",
    "    text = remove_ats_hashtags_and_links(text)\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    tokenlist = nltk.word_tokenize(text)\n",
    "    # tokenlist = remove_stopwords(tokenlist)\n",
    "    # lemmatizer = nltk.WordNetLemmatizer()\n",
    "    # lemmalist = [lemmatizer.lemmatize(token) for token in tokenlist]\n",
    "    tokenlist = remove_nonalpha(tokenlist)\n",
    "    text = \"\"\n",
    "    for tkn in tokenlist :\n",
    "        text += tkn + \" \"\n",
    "    text = text[ : -1]\n",
    "    if text == \"\" : return \" \" # Treat NaN\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twiter Airlines\n",
    "dataset_folder = \"../resources/datasets/TwitterAirlines\"\n",
    "\n",
    "dataset = pd.read_csv(f\"{dataset_folder}/Tweets.csv\", index_col=0)\n",
    "\n",
    "columns_to_keep = [\"airline_sentiment\", \"airline_sentiment_confidence\", \"text\"]\n",
    "dataset_clean = dataset.iloc[ : , [column in columns_to_keep for column in dataset.columns]]\n",
    "dataset_clean.to_csv(f\"{dataset_folder}/TweetsDroppedCols.csv\")\n",
    "\n",
    "dataset_clean = dataset_clean.copy() # Defrag\n",
    "\n",
    "processed_text = []\n",
    "classes = []\n",
    "index = []\n",
    "for i, (_, row) in enumerate(dataset_clean.iterrows()) :\n",
    "    if i % 100 == 0 : \n",
    "        print(i)\n",
    "    processed = preprocess(row[\"text\"])\n",
    "    if processed == \"\" : processed = \" \"\n",
    "    processed_text.append(processed)\n",
    "    classes.append([\"negative\", \"neutral\", \"positive\"].index(row[\"airline_sentiment\"]))\n",
    "    index.append(row.name)\n",
    "processed_text_column = pd.Series(processed_text, index, name=\"text\")\n",
    "classes_column = pd.Series(classes, index, name=\"class\")\n",
    "\n",
    "dataset_processed = pd.concat([processed_text_column, classes_column], axis=1)\n",
    "dataset_processed.to_csv(f\"{dataset_folder}/TweetsProcessed.csv\")\n",
    "dataset_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>splitset</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the rock is destined to be the 21st century s ...</td>\n",
       "      <td>226166</td>\n",
       "      <td>1</td>\n",
       "      <td>0.69444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the gorgeously elaborate continuation of the l...</td>\n",
       "      <td>226300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.83333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>effective but too tepid biopic</td>\n",
       "      <td>13995</td>\n",
       "      <td>2</td>\n",
       "      <td>0.51389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>14123</td>\n",
       "      <td>2</td>\n",
       "      <td>0.73611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>emerges as something rare an issue movie that ...</td>\n",
       "      <td>13999</td>\n",
       "      <td>2</td>\n",
       "      <td>0.86111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11851</th>\n",
       "      <td>a real snooze</td>\n",
       "      <td>222071</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11852</th>\n",
       "      <td>no surprises</td>\n",
       "      <td>225165</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11853</th>\n",
       "      <td>we ve seen the hippie turned yuppie plot befor...</td>\n",
       "      <td>226985</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11854</th>\n",
       "      <td>her fans walked out muttering words like horri...</td>\n",
       "      <td>223632</td>\n",
       "      <td>1</td>\n",
       "      <td>0.13889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11855</th>\n",
       "      <td>in this case zero</td>\n",
       "      <td>224044</td>\n",
       "      <td>1</td>\n",
       "      <td>0.34722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11855 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  phrase_id  splitset  \\\n",
       "1      the rock is destined to be the 21st century s ...     226166         1   \n",
       "2      the gorgeously elaborate continuation of the l...     226300         1   \n",
       "3                         effective but too tepid biopic      13995         2   \n",
       "4      if you sometimes like to go to the movies to h...      14123         2   \n",
       "5      emerges as something rare an issue movie that ...      13999         2   \n",
       "...                                                  ...        ...       ...   \n",
       "11851                                      a real snooze     222071         1   \n",
       "11852                                       no surprises     225165         1   \n",
       "11853  we ve seen the hippie turned yuppie plot befor...     226985         1   \n",
       "11854  her fans walked out muttering words like horri...     223632         1   \n",
       "11855                                  in this case zero     224044         1   \n",
       "\n",
       "         class  \n",
       "1      0.69444  \n",
       "2      0.83333  \n",
       "3      0.51389  \n",
       "4      0.73611  \n",
       "5      0.86111  \n",
       "...        ...  \n",
       "11851  0.11111  \n",
       "11852  0.22222  \n",
       "11853  0.75000  \n",
       "11854  0.13889  \n",
       "11855  0.34722  \n",
       "\n",
       "[11855 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stanford Sentiment Treebank 2 e IMDB Movie Reviews\n",
    "# Args\n",
    "# dataset_folder = \"../resources/datasets/IMDBMovieReviews\"\n",
    "# dataset_name = \"IMDB_MR\"\n",
    "dataset_folder = \"../resources/datasets/StanfordSentimentTreebank\"\n",
    "dataset_name = \"SST2\"\n",
    "# Exec\n",
    "dataset = pd.read_csv(f\"{dataset_folder}/{dataset_name}.csv\", index_col=0)\n",
    "\n",
    "processed_text = []\n",
    "index = []\n",
    "for i, (_, row) in enumerate(dataset.iterrows()) :\n",
    "    if i % 100 == 0 : \n",
    "        print(i)\n",
    "    processed = preprocess(row[\"text\"])\n",
    "    if processed == \"\" : processed = \" \"\n",
    "    processed_text.append(processed)\n",
    "    index.append(row.name)\n",
    "processed_text_column = pd.Series(processed_text, index, name=\"text\")\n",
    "\n",
    "dataset_processed = dataset.copy()\n",
    "dataset_processed[\"text\"] = processed_text_column\n",
    "dataset_processed.to_csv(f\"{dataset_folder}/{dataset_name}Processed2.csv\")\n",
    "dataset_processed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb071969cc081d156db4658a52cc12ff3302089485c0c8cb524fcf02c6362775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
