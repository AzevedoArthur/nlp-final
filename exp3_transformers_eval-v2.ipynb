{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\T-Gamer\\\\Documents\\\\SideDrive\\\\UFMA\\\\2022.1\\\\Topicos Especiais (NLP)\\\\Exercicios\\\\Trabalho Final\\\\Implementação\\\\source'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, json, math\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_word_index(sentences) :\n",
    "    all_tokens = []\n",
    "    for txt in sentences :\n",
    "        all_tokens += txt.split()\n",
    "\n",
    "    tokens = pd.Series(all_tokens, range(len(all_tokens)), name=\"tokens\")\n",
    "    types = tokens.unique()\n",
    "    word_index = {word : i for i, word in enumerate([\"<pad>\", \"<unk>\"] + list(types))}\n",
    "    \n",
    "    def decode_review(text):\n",
    "        nonlocal word_index\n",
    "        reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "        return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "    \n",
    "    return word_index, decode_review\n",
    "\n",
    "def get_dataset(dataframe, word_index, max_len=256) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    x_series, y_series = dataframe[\"text\"], dataframe[\"class\"]\n",
    "\n",
    "    x_list = [txt.split() for txt in list(x_series)]\n",
    "    \n",
    "    x_seq = []\n",
    "    for tknlst in x_list :\n",
    "        seq = []\n",
    "        for tkn in tknlst :\n",
    "            try :\n",
    "                seq.append(word_index[tkn])\n",
    "            except KeyError :\n",
    "                seq.append(word_index[\"<unk>\"])\n",
    "        seq = (seq + [0] * (max_len - len(seq))) if (len(seq) < max_len) else (seq[ : max_len])\n",
    "        x_seq.append(seq)\n",
    "    \n",
    "    # y_int = [1 if lb >= .5 else 0 for lb in list(y_series)]\n",
    "\n",
    "    x = np.array(x_seq, dtype=int) \n",
    "    mask = x != 0\n",
    "\n",
    "    y = np.array(list(y_series), dtype=float) \n",
    "\n",
    "    return x, mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2210, 256) (2210, 256) (2210,)\n",
      "2210\n",
      "{'input_ids': tensor([2, 3, 4, 5, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0.5139)}\n"
     ]
    }
   ],
   "source": [
    "# Para dataset simples, sem folding\n",
    "# Args\n",
    "resource_folder = \"../resources\"\n",
    "dataset_folder = \"StanfordSentimentTreebank\"\n",
    "dataset_name   = \"SST2Processed2\"\n",
    "dataset_split   = \"test\"\n",
    "experiment_name = f\"{dataset_name}-train-hard\"\n",
    "n_classes = 1\n",
    "batch_size = 16\n",
    "\n",
    "# Exec\n",
    "embeddings_df = pd.read_csv(f\"{resource_folder}/embeddings/{dataset_folder}/{dataset_name}-train_dim768.csv\",        index_col=0)\n",
    "dataset_df    = pd.read_csv(f\"{resource_folder}/datasets/{dataset_folder}/split/{dataset_name}-{dataset_split}.csv\", index_col=0)\n",
    "\n",
    "output_dir    = f\"../resources/output/{experiment_name}-Eval\"\n",
    "\n",
    "word_index, decode_review = get_word_index(dataset_df[\"text\"])\n",
    "x_test, mask_test, y_test = get_dataset(dataset_df, word_index)\n",
    "print(x_test.shape, mask_test.shape, y_test.shape)\n",
    "dataset = []\n",
    "for x, msk, y in zip(x_test, mask_test, y_test) : \n",
    "    x   = torch.tensor([int(tkn) for tkn in x])\n",
    "    msk = torch.tensor([int(tkn) for tkn in msk])\n",
    "    y   = torch.tensor(float(y))\n",
    "    dataset.append({'input_ids'      : x,\n",
    "                    'attention_mask' : msk,\n",
    "                    'labels'         : y}) \n",
    "print(len(dataset))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    f\"{resource_folder}/output/{experiment_name}/final\",\n",
    "    num_labels=n_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(16177, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 of 139\n",
      "Batch 2 of 139\n",
      "Batch 3 of 139\n",
      "Batch 4 of 139\n",
      "Batch 5 of 139\n",
      "Batch 6 of 139\n",
      "Batch 7 of 139\n",
      "Batch 8 of 139\n",
      "Batch 9 of 139\n",
      "Batch 10 of 139\n",
      "Batch 11 of 139\n",
      "Batch 12 of 139\n",
      "Batch 13 of 139\n",
      "Batch 14 of 139\n",
      "Batch 15 of 139\n",
      "Batch 16 of 139\n",
      "Batch 17 of 139\n",
      "Batch 18 of 139\n",
      "Batch 19 of 139\n",
      "Batch 20 of 139\n",
      "Batch 21 of 139\n",
      "Batch 22 of 139\n",
      "Batch 23 of 139\n",
      "Batch 24 of 139\n",
      "Batch 25 of 139\n",
      "Batch 26 of 139\n",
      "Batch 27 of 139\n",
      "Batch 28 of 139\n",
      "Batch 29 of 139\n",
      "Batch 30 of 139\n",
      "Batch 31 of 139\n",
      "Batch 32 of 139\n",
      "Batch 33 of 139\n",
      "Batch 34 of 139\n",
      "Batch 35 of 139\n",
      "Batch 36 of 139\n",
      "Batch 37 of 139\n",
      "Batch 38 of 139\n",
      "Batch 39 of 139\n",
      "Batch 40 of 139\n",
      "Batch 41 of 139\n",
      "Batch 42 of 139\n",
      "Batch 43 of 139\n",
      "Batch 44 of 139\n",
      "Batch 45 of 139\n",
      "Batch 46 of 139\n",
      "Batch 47 of 139\n",
      "Batch 48 of 139\n",
      "Batch 49 of 139\n",
      "Batch 50 of 139\n",
      "Batch 51 of 139\n",
      "Batch 52 of 139\n",
      "Batch 53 of 139\n",
      "Batch 54 of 139\n",
      "Batch 55 of 139\n",
      "Batch 56 of 139\n",
      "Batch 57 of 139\n",
      "Batch 58 of 139\n",
      "Batch 59 of 139\n",
      "Batch 60 of 139\n",
      "Batch 61 of 139\n",
      "Batch 62 of 139\n",
      "Batch 63 of 139\n",
      "Batch 64 of 139\n",
      "Batch 65 of 139\n",
      "Batch 66 of 139\n",
      "Batch 67 of 139\n",
      "Batch 68 of 139\n",
      "Batch 69 of 139\n",
      "Batch 70 of 139\n",
      "Batch 71 of 139\n",
      "Batch 72 of 139\n",
      "Batch 73 of 139\n",
      "Batch 74 of 139\n",
      "Batch 75 of 139\n",
      "Batch 76 of 139\n",
      "Batch 77 of 139\n",
      "Batch 78 of 139\n",
      "Batch 79 of 139\n",
      "Batch 80 of 139\n",
      "Batch 81 of 139\n",
      "Batch 82 of 139\n",
      "Batch 83 of 139\n",
      "Batch 84 of 139\n",
      "Batch 85 of 139\n",
      "Batch 86 of 139\n",
      "Batch 87 of 139\n",
      "Batch 88 of 139\n",
      "Batch 89 of 139\n",
      "Batch 90 of 139\n",
      "Batch 91 of 139\n",
      "Batch 92 of 139\n",
      "Batch 93 of 139\n",
      "Batch 94 of 139\n",
      "Batch 95 of 139\n",
      "Batch 96 of 139\n",
      "Batch 97 of 139\n",
      "Batch 98 of 139\n",
      "Batch 99 of 139\n",
      "Batch 100 of 139\n",
      "Batch 101 of 139\n",
      "Batch 102 of 139\n",
      "Batch 103 of 139\n",
      "Batch 104 of 139\n",
      "Batch 105 of 139\n",
      "Batch 106 of 139\n",
      "Batch 107 of 139\n",
      "Batch 108 of 139\n",
      "Batch 109 of 139\n",
      "Batch 110 of 139\n",
      "Batch 111 of 139\n",
      "Batch 112 of 139\n",
      "Batch 113 of 139\n",
      "Batch 114 of 139\n",
      "Batch 115 of 139\n",
      "Batch 116 of 139\n",
      "Batch 117 of 139\n",
      "Batch 118 of 139\n",
      "Batch 119 of 139\n",
      "Batch 120 of 139\n",
      "Batch 121 of 139\n",
      "Batch 122 of 139\n",
      "Batch 123 of 139\n",
      "Batch 124 of 139\n",
      "Batch 125 of 139\n",
      "Batch 126 of 139\n",
      "Batch 127 of 139\n",
      "Batch 128 of 139\n",
      "Batch 129 of 139\n",
      "Batch 130 of 139\n",
      "Batch 131 of 139\n",
      "Batch 132 of 139\n",
      "Batch 133 of 139\n",
      "Batch 134 of 139\n",
      "Batch 135 of 139\n",
      "Batch 136 of 139\n",
      "Batch 137 of 139\n",
      "Batch 138 of 139\n",
      "Batch 139 of 139\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "truths = []\n",
    "n_batches = math.ceil(len(dataset) / batch_size)\n",
    "for i_batch in range(n_batches) :\n",
    "    print(\"Batch\", i_batch + 1, \"of\", n_batches)\n",
    "    batch = dataset[i_batch * batch_size : (i_batch + 1) * batch_size]\n",
    "    \n",
    "    input_dict = {\n",
    "        'input_ids'      : torch.tensor([list(sentence['input_ids']) for sentence in batch]), \n",
    "        'attention_mask' : torch.tensor([list(sentence['attention_mask']) for sentence in batch]),\n",
    "        'labels' : torch.tensor([sentence['labels'] for sentence in batch])\n",
    "    }\n",
    "    \n",
    "    preds = model.forward(input_ids      = input_dict['input_ids'], \n",
    "                          attention_mask = input_dict['attention_mask'])\n",
    "    predictions  += [float(logit[0]) for logit in preds['logits']]\n",
    "    truths       += input_dict['labels']\n",
    "prediction_column = pd.Series(predictions, dataset_df.index, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_preds_df = pd.concat([dataset_df, prediction_column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir) :\n",
    "    os.makedirs(output_dir)\n",
    "dataset_with_preds_df.to_csv(f\"{output_dir}/Predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>class</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; effective but too tepid biopic</td>\n",
       "      <td>13995</td>\n",
       "      <td>0.513890</td>\n",
       "      <td>0.813343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; if you sometimes like to go to the mov...</td>\n",
       "      <td>14123</td>\n",
       "      <td>0.736110</td>\n",
       "      <td>0.729453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;start&gt; emerges as something rare an issue mov...</td>\n",
       "      <td>13999</td>\n",
       "      <td>0.861110</td>\n",
       "      <td>0.898561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;start&gt; the film provides some great insight i...</td>\n",
       "      <td>14498</td>\n",
       "      <td>0.597220</td>\n",
       "      <td>0.874723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;start&gt; offers that rare combination of entert...</td>\n",
       "      <td>14351</td>\n",
       "      <td>0.833330</td>\n",
       "      <td>0.904525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11621</th>\n",
       "      <td>&lt;start&gt; an imaginative comedythriller</td>\n",
       "      <td>13851</td>\n",
       "      <td>0.777780</td>\n",
       "      <td>0.530839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11623</th>\n",
       "      <td>&lt;start&gt; a rare beautiful film</td>\n",
       "      <td>18182</td>\n",
       "      <td>0.916670</td>\n",
       "      <td>0.939123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11626</th>\n",
       "      <td>&lt;start&gt; an hilarious romantic comedy</td>\n",
       "      <td>23211</td>\n",
       "      <td>0.888890</td>\n",
       "      <td>0.948151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11628</th>\n",
       "      <td>&lt;start&gt; never sinks into exploitation</td>\n",
       "      <td>26177</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.731718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11798</th>\n",
       "      <td>&lt;start&gt; u nrelentingly stupid</td>\n",
       "      <td>141831</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>0.680484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2210 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  phrase_id     class  \\\n",
       "3                 <start> effective but too tepid biopic      13995  0.513890   \n",
       "4      <start> if you sometimes like to go to the mov...      14123  0.736110   \n",
       "5      <start> emerges as something rare an issue mov...      13999  0.861110   \n",
       "6      <start> the film provides some great insight i...      14498  0.597220   \n",
       "7      <start> offers that rare combination of entert...      14351  0.833330   \n",
       "...                                                  ...        ...       ...   \n",
       "11621              <start> an imaginative comedythriller      13851  0.777780   \n",
       "11623                      <start> a rare beautiful film      18182  0.916670   \n",
       "11626               <start> an hilarious romantic comedy      23211  0.888890   \n",
       "11628              <start> never sinks into exploitation      26177  0.625000   \n",
       "11798                      <start> u nrelentingly stupid     141831  0.069444   \n",
       "\n",
       "       predictions  \n",
       "3         0.813343  \n",
       "4         0.729453  \n",
       "5         0.898561  \n",
       "6         0.874723  \n",
       "7         0.904525  \n",
       "...            ...  \n",
       "11621     0.530839  \n",
       "11623     0.939123  \n",
       "11626     0.948151  \n",
       "11628     0.731718  \n",
       "11798     0.680484  \n",
       "\n",
       "[2210 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5579185520361991"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits, total = 0, 0\n",
    "for _, row in dataset_with_preds_df.iterrows() :\n",
    "    total += 1\n",
    "    pred  = row[\"predictions\"] >= .5\n",
    "    label = row[\"class\"]       >= .5\n",
    "    if pred == label : \n",
    "        hits += 1\n",
    "accuracy = hits / total\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32979248465010524"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits, total = 0, 0\n",
    "for _, row in dataset_with_preds_df.iterrows() :\n",
    "    total += 1\n",
    "    pred  = row[\"predictions\"]\n",
    "    label = row[\"class\"]\n",
    "    diff = label - pred\n",
    "    hits += abs(diff)\n",
    "avg_dev = hits / total\n",
    "avg_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921 312 787 190\n"
     ]
    }
   ],
   "source": [
    "true_positives, true_negatives, false_positives, false_negatives = 0, 0, 0, 0\n",
    "for _, row in dataset_with_preds_df.iterrows() :\n",
    "    pred  = row[\"predictions\"] >= .5\n",
    "    label = row[\"class\"]       >= .5\n",
    "    if pred == label :\n",
    "        if pred :\n",
    "            true_positives  += 1\n",
    "        else :\n",
    "            true_negatives  += 1\n",
    "    else :\n",
    "        if pred :\n",
    "            false_positives += 1\n",
    "        else :\n",
    "            false_negatives += 1\n",
    "print(true_positives, true_negatives, false_positives, false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5392271662763466"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = true_positives / (true_positives + false_positives)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.828982898289829"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = true_positives / (true_positives + false_negatives)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6534231997162114"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = (2 * precision * recall) / (precision + recall)\n",
    "f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb071969cc081d156db4658a52cc12ff3302089485c0c8cb524fcf02c6362775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
