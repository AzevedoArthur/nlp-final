{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer_pt_utils.py:195: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  device: Optional[torch.device] = torch.device(\"cuda\"),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\T-Gamer\\\\Documents\\\\SideDrive\\\\UFMA\\\\2022.1\\\\Topicos Especiais (NLP)\\\\Exercicios\\\\Trabalho Final\\\\Implementação\\\\source'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DistilBertForSequenceClassification, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_word_index(sentences) :\n",
    "    all_tokens = []\n",
    "    for txt in sentences :\n",
    "        all_tokens += txt.split()\n",
    "\n",
    "    tokens = pd.Series(all_tokens, range(len(all_tokens)), name=\"tokens\")\n",
    "    types = tokens.unique()\n",
    "    word_index = {word : i for i, word in enumerate([\"<pad>\", \"<unk>\"] + list(types))}\n",
    "    \n",
    "    def decode_review(text):\n",
    "        nonlocal word_index\n",
    "        reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "        return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "    \n",
    "    return word_index, decode_review\n",
    "\n",
    "def get_dataset(dataframe, word_index, max_len=256) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    x_series, y_series = dataframe[\"text\"], dataframe[\"class\"]\n",
    "\n",
    "    x_list = [txt.split() for txt in list(x_series)]\n",
    "    \n",
    "    x_seq = []\n",
    "    for tknlst in x_list :\n",
    "        seq = []\n",
    "        for tkn in tknlst :\n",
    "            try :\n",
    "                seq.append(word_index[tkn])\n",
    "            except KeyError :\n",
    "                seq.append(word_index[\"<unk>\"])\n",
    "        seq = (seq + [0] * (max_len - len(seq))) if (len(seq) < max_len) else (seq[ : max_len])\n",
    "        x_seq.append(seq)\n",
    "    \n",
    "    # y_int = [1 if lb >= .5 else 0 for lb in list(y_series)]\n",
    "\n",
    "    x = np.array(x_seq, dtype=int) \n",
    "    mask = x != 0\n",
    "\n",
    "    y = np.array(list(y_series), dtype=float) \n",
    "\n",
    "    return x, mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, n_classes, embeddings:pd.DataFrame, epochs:int, output_dir:str, logging_dir:str) -> DistilBertForSequenceClassification :\n",
    "    global resume_from_checkpoint\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        num_labels=n_classes\n",
    "    )\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    embeddings_np_array = embeddings.values\n",
    "    embeddings_np_array = np.append(np.zeros((2, embeddings_np_array.shape[1])), embeddings_np_array, axis=0)\n",
    "    embeddings_module = torch.nn.Embedding.from_pretrained(\n",
    "        embeddings=torch.tensor(embeddings_np_array),\n",
    "        padding_idx=0\n",
    "    )\n",
    "    model.set_input_embeddings(embeddings_module)\n",
    "    model.config.vocab_size = embeddings_np_array.shape[0]\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=logging_dir,\n",
    "        logging_steps=10\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset\n",
    "    )\n",
    "    trainer.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8544, 256) (8544, 256) (8544,)\n",
      "8544\n",
      "{'input_ids': tensor([ 2,  3,  4,  5,  6,  7,  8,  3,  9, 10, 11, 12, 13, 14, 15, 16,  7, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0.6944)}\n"
     ]
    }
   ],
   "source": [
    "# Para dataset simples, sem folding\n",
    "# Args\n",
    "resource_folder = \"../resources\"\n",
    "dataset_folder = \"StanfordSentimentTreebank\"\n",
    "dataset_name   = \"SST2Processed2-train\"\n",
    "n_classes = 1\n",
    "\n",
    "# Exec\n",
    "embeddings_df = pd.read_csv(f\"{resource_folder}/embeddings/{dataset_folder}/{dataset_name}_dim768.csv\",     index_col=0)\n",
    "dataset_df    = pd.read_csv(f\"{resource_folder}/datasets/{dataset_folder}/split/{dataset_name}.csv\", index_col=0)\n",
    "output_dir    = f\"{resource_folder}/output/{dataset_name}\"\n",
    "logging_dir   = f\"{resource_folder}/logs/{dataset_name}\"\n",
    "\n",
    "word_index, decode_review = get_word_index(dataset_df[\"text\"])\n",
    "x_train, mask_train, y_train = get_dataset(dataset_df, word_index)\n",
    "print(x_train.shape, mask_train.shape, y_train.shape)\n",
    "dataset = []\n",
    "for x, msk, y in zip(x_train, mask_train, y_train) : \n",
    "    x   = torch.tensor([int(tkn) for tkn in x])\n",
    "    msk = torch.tensor([int(tkn) for tkn in msk])\n",
    "    y   = torch.tensor(float(y))\n",
    "    dataset.append({'input_ids'      : x,\n",
    "                    'attention_mask' : msk,\n",
    "                    'labels'         : y}) \n",
    "print(len(dataset))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> the rock is destined to be the 21st century's new conan and that he's going to make a splash even greater than arnold schwarzenegger jean claud van damme or steven segal <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(decode_review([int(id) for id in dataset[0]['input_ids']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2670\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77417df3933d4f74a74684146debb74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2670 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2839, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.02}\n",
      "{'loss': 0.1955, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.04}\n",
      "{'loss': 0.0977, 'learning_rate': 3e-06, 'epoch': 0.06}\n",
      "{'loss': 0.0757, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.07}\n",
      "{'loss': 0.0651, 'learning_rate': 5e-06, 'epoch': 0.09}\n",
      "{'loss': 0.0585, 'learning_rate': 6e-06, 'epoch': 0.11}\n",
      "{'loss': 0.0748, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.13}\n",
      "{'loss': 0.0597, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.15}\n",
      "{'loss': 0.0668, 'learning_rate': 9e-06, 'epoch': 0.17}\n",
      "{'loss': 0.0658, 'learning_rate': 1e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0551, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0598, 'learning_rate': 1.2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0614, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0561, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.26}\n",
      "{'loss': 0.066, 'learning_rate': 1.5e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0631, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0649, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0608, 'learning_rate': 1.8e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0684, 'learning_rate': 1.9e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0706, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0569, 'learning_rate': 2.1e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0601, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0594, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0552, 'learning_rate': 2.4e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0652, 'learning_rate': 2.5e-05, 'epoch': 0.47}\n",
      "{'loss': 0.061, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0667, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0603, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0525, 'learning_rate': 2.9e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0574, 'learning_rate': 3e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0606, 'learning_rate': 3.1e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0524, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0575, 'learning_rate': 3.3e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0586, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0642, 'learning_rate': 3.5e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0501, 'learning_rate': 3.6e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0584, 'learning_rate': 3.7e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0611, 'learning_rate': 3.8e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0603, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0678, 'learning_rate': 4e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0587, 'learning_rate': 4.1e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0593, 'learning_rate': 4.2e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0585, 'learning_rate': 4.3e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0595, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0642, 'learning_rate': 4.5e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0482, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0559, 'learning_rate': 4.7e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0514, 'learning_rate': 4.8e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0499, 'learning_rate': 4.9e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../resources/output/SST2Processed2-train\\checkpoint-500\n",
      "Configuration saved in ../resources/output/SST2Processed2-train\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0558, 'learning_rate': 5e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../resources/output/SST2Processed2-train\\checkpoint-500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0576, 'learning_rate': 4.976958525345622e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0594, 'learning_rate': 4.9539170506912444e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0691, 'learning_rate': 4.9308755760368664e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0508, 'learning_rate': 4.9078341013824885e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0538, 'learning_rate': 4.8847926267281106e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0525, 'learning_rate': 4.861751152073733e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0621, 'learning_rate': 4.8387096774193554e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0477, 'learning_rate': 4.8156682027649774e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0688, 'learning_rate': 4.792626728110599e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0619, 'learning_rate': 4.7695852534562216e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0523, 'learning_rate': 4.7465437788018436e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0464, 'learning_rate': 4.723502304147466e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0476, 'learning_rate': 4.700460829493088e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0566, 'learning_rate': 4.67741935483871e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0572, 'learning_rate': 4.654377880184332e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0576, 'learning_rate': 4.631336405529954e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0479, 'learning_rate': 4.608294930875576e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0551, 'learning_rate': 4.585253456221199e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0577, 'learning_rate': 4.562211981566821e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0464, 'learning_rate': 4.539170506912442e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0526, 'learning_rate': 4.516129032258064e-05, 'epoch': 1.33}\n",
      "{'loss': 0.056, 'learning_rate': 4.493087557603687e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0556, 'learning_rate': 4.470046082949309e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0457, 'learning_rate': 4.447004608294931e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0464, 'learning_rate': 4.423963133640553e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0484, 'learning_rate': 4.400921658986175e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0454, 'learning_rate': 4.3778801843317974e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0506, 'learning_rate': 4.3548387096774194e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0552, 'learning_rate': 4.3317972350230415e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0568, 'learning_rate': 4.308755760368664e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0518, 'learning_rate': 4.2857142857142856e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0535, 'learning_rate': 4.262672811059908e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0514, 'learning_rate': 4.2396313364055304e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0496, 'learning_rate': 4.2165898617511525e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0449, 'learning_rate': 4.1935483870967746e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0505, 'learning_rate': 4.1705069124423966e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0471, 'learning_rate': 4.147465437788019e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0598, 'learning_rate': 4.124423963133641e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0522, 'learning_rate': 4.101382488479263e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0504, 'learning_rate': 4.078341013824885e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0445, 'learning_rate': 4.0552995391705076e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0565, 'learning_rate': 4.032258064516129e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0475, 'learning_rate': 4.009216589861751e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0413, 'learning_rate': 3.986175115207373e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0464, 'learning_rate': 3.963133640552996e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0496, 'learning_rate': 3.940092165898618e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0437, 'learning_rate': 3.91705069124424e-05, 'epoch': 1.82}\n",
      "{'loss': 0.0525, 'learning_rate': 3.8940092165898614e-05, 'epoch': 1.84}\n",
      "{'loss': 0.0431, 'learning_rate': 3.870967741935484e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../resources/output/SST2Processed2-train\\checkpoint-1000\n",
      "Configuration saved in ../resources/output/SST2Processed2-train\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0422, 'learning_rate': 3.847926267281106e-05, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../resources/output/SST2Processed2-train\\checkpoint-1000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.05, 'learning_rate': 3.824884792626728e-05, 'epoch': 1.89}\n",
      "{'loss': 0.0432, 'learning_rate': 3.8018433179723504e-05, 'epoch': 1.91}\n",
      "{'loss': 0.0481, 'learning_rate': 3.7788018433179724e-05, 'epoch': 1.93}\n",
      "{'loss': 0.0434, 'learning_rate': 3.7557603686635945e-05, 'epoch': 1.95}\n",
      "{'loss': 0.0475, 'learning_rate': 3.7327188940092166e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0431, 'learning_rate': 3.7096774193548386e-05, 'epoch': 1.99}\n",
      "{'loss': 0.0449, 'learning_rate': 3.6866359447004614e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0365, 'learning_rate': 3.6635944700460834e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0356, 'learning_rate': 3.640552995391705e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0406, 'learning_rate': 3.6175115207373276e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0403, 'learning_rate': 3.5944700460829496e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0354, 'learning_rate': 3.571428571428572e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0422, 'learning_rate': 3.548387096774194e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0424, 'learning_rate': 3.525345622119816e-05, 'epoch': 2.13}\n",
      "{'loss': 0.045, 'learning_rate': 3.502304147465438e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0336, 'learning_rate': 3.47926267281106e-05, 'epoch': 2.17}\n",
      "{'loss': 0.0399, 'learning_rate': 3.456221198156682e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0475, 'learning_rate': 3.433179723502305e-05, 'epoch': 2.21}\n",
      "{'loss': 0.0419, 'learning_rate': 3.410138248847927e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0495, 'learning_rate': 3.387096774193548e-05, 'epoch': 2.25}\n",
      "{'loss': 0.0369, 'learning_rate': 3.36405529953917e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0409, 'learning_rate': 3.341013824884793e-05, 'epoch': 2.28}\n",
      "{'loss': 0.037, 'learning_rate': 3.317972350230415e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0332, 'learning_rate': 3.294930875576037e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0388, 'learning_rate': 3.271889400921659e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0387, 'learning_rate': 3.248847926267281e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0398, 'learning_rate': 3.2258064516129034e-05, 'epoch': 2.38}\n",
      "{'loss': 0.048, 'learning_rate': 3.2027649769585254e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0478, 'learning_rate': 3.1797235023041475e-05, 'epoch': 2.42}\n",
      "{'loss': 0.0332, 'learning_rate': 3.15668202764977e-05, 'epoch': 2.43}\n",
      "{'loss': 0.0411, 'learning_rate': 3.1336405529953916e-05, 'epoch': 2.45}\n",
      "{'loss': 0.0434, 'learning_rate': 3.110599078341014e-05, 'epoch': 2.47}\n",
      "{'loss': 0.0392, 'learning_rate': 3.087557603686636e-05, 'epoch': 2.49}\n",
      "{'loss': 0.0412, 'learning_rate': 3.0645161290322585e-05, 'epoch': 2.51}\n",
      "{'loss': 0.0381, 'learning_rate': 3.0414746543778806e-05, 'epoch': 2.53}\n",
      "{'loss': 0.046, 'learning_rate': 3.0184331797235026e-05, 'epoch': 2.55}\n",
      "{'loss': 0.0455, 'learning_rate': 2.9953917050691244e-05, 'epoch': 2.57}\n",
      "{'loss': 0.0411, 'learning_rate': 2.9723502304147464e-05, 'epoch': 2.58}\n",
      "{'loss': 0.0414, 'learning_rate': 2.9493087557603688e-05, 'epoch': 2.6}\n",
      "{'loss': 0.0488, 'learning_rate': 2.926267281105991e-05, 'epoch': 2.62}\n",
      "{'loss': 0.0419, 'learning_rate': 2.9032258064516133e-05, 'epoch': 2.64}\n",
      "{'loss': 0.034, 'learning_rate': 2.880184331797235e-05, 'epoch': 2.66}\n",
      "{'loss': 0.0364, 'learning_rate': 2.857142857142857e-05, 'epoch': 2.68}\n",
      "{'loss': 0.0423, 'learning_rate': 2.8341013824884795e-05, 'epoch': 2.7}\n",
      "{'loss': 0.049, 'learning_rate': 2.8110599078341016e-05, 'epoch': 2.72}\n",
      "{'loss': 0.0397, 'learning_rate': 2.7880184331797236e-05, 'epoch': 2.73}\n",
      "{'loss': 0.0442, 'learning_rate': 2.764976958525346e-05, 'epoch': 2.75}\n",
      "{'loss': 0.0328, 'learning_rate': 2.7419354838709678e-05, 'epoch': 2.77}\n",
      "{'loss': 0.0363, 'learning_rate': 2.7188940092165898e-05, 'epoch': 2.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../resources/output/SST2Processed2-train\\checkpoint-1500\n",
      "Configuration saved in ../resources/output/SST2Processed2-train\\checkpoint-1500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0411, 'learning_rate': 2.6958525345622122e-05, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../resources/output/SST2Processed2-train\\checkpoint-1500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0398, 'learning_rate': 2.6728110599078343e-05, 'epoch': 2.83}\n",
      "{'loss': 0.0428, 'learning_rate': 2.6497695852534567e-05, 'epoch': 2.85}\n",
      "{'loss': 0.035, 'learning_rate': 2.626728110599078e-05, 'epoch': 2.87}\n",
      "{'loss': 0.0423, 'learning_rate': 2.6036866359447005e-05, 'epoch': 2.88}\n",
      "{'loss': 0.0301, 'learning_rate': 2.5806451612903226e-05, 'epoch': 2.9}\n",
      "{'loss': 0.0317, 'learning_rate': 2.557603686635945e-05, 'epoch': 2.92}\n",
      "{'loss': 0.0442, 'learning_rate': 2.534562211981567e-05, 'epoch': 2.94}\n",
      "{'loss': 0.0445, 'learning_rate': 2.5115207373271894e-05, 'epoch': 2.96}\n",
      "{'loss': 0.0421, 'learning_rate': 2.488479262672811e-05, 'epoch': 2.98}\n",
      "{'loss': 0.0349, 'learning_rate': 2.4654377880184332e-05, 'epoch': 3.0}\n",
      "{'loss': 0.0332, 'learning_rate': 2.4423963133640553e-05, 'epoch': 3.01}\n",
      "{'loss': 0.0324, 'learning_rate': 2.4193548387096777e-05, 'epoch': 3.03}\n",
      "{'loss': 0.032, 'learning_rate': 2.3963133640552994e-05, 'epoch': 3.05}\n",
      "{'loss': 0.0346, 'learning_rate': 2.3732718894009218e-05, 'epoch': 3.07}\n",
      "{'loss': 0.0297, 'learning_rate': 2.350230414746544e-05, 'epoch': 3.09}\n",
      "{'loss': 0.0314, 'learning_rate': 2.327188940092166e-05, 'epoch': 3.11}\n",
      "{'loss': 0.028, 'learning_rate': 2.304147465437788e-05, 'epoch': 3.13}\n",
      "{'loss': 0.0273, 'learning_rate': 2.2811059907834104e-05, 'epoch': 3.15}\n",
      "{'loss': 0.0282, 'learning_rate': 2.258064516129032e-05, 'epoch': 3.16}\n",
      "{'loss': 0.0314, 'learning_rate': 2.2350230414746546e-05, 'epoch': 3.18}\n",
      "{'loss': 0.0283, 'learning_rate': 2.2119815668202766e-05, 'epoch': 3.2}\n",
      "{'loss': 0.0293, 'learning_rate': 2.1889400921658987e-05, 'epoch': 3.22}\n",
      "{'loss': 0.0329, 'learning_rate': 2.1658986175115207e-05, 'epoch': 3.24}\n",
      "{'loss': 0.0292, 'learning_rate': 2.1428571428571428e-05, 'epoch': 3.26}\n",
      "{'loss': 0.0331, 'learning_rate': 2.1198156682027652e-05, 'epoch': 3.28}\n",
      "{'loss': 0.0303, 'learning_rate': 2.0967741935483873e-05, 'epoch': 3.3}\n",
      "{'loss': 0.0333, 'learning_rate': 2.0737327188940094e-05, 'epoch': 3.31}\n",
      "{'loss': 0.0271, 'learning_rate': 2.0506912442396314e-05, 'epoch': 3.33}\n",
      "{'loss': 0.0298, 'learning_rate': 2.0276497695852538e-05, 'epoch': 3.35}\n",
      "{'loss': 0.0341, 'learning_rate': 2.0046082949308755e-05, 'epoch': 3.37}\n",
      "{'loss': 0.0318, 'learning_rate': 1.981566820276498e-05, 'epoch': 3.39}\n",
      "{'loss': 0.0317, 'learning_rate': 1.95852534562212e-05, 'epoch': 3.41}\n",
      "{'loss': 0.0294, 'learning_rate': 1.935483870967742e-05, 'epoch': 3.43}\n",
      "{'loss': 0.038, 'learning_rate': 1.912442396313364e-05, 'epoch': 3.45}\n",
      "{'loss': 0.028, 'learning_rate': 1.8894009216589862e-05, 'epoch': 3.46}\n",
      "{'loss': 0.0331, 'learning_rate': 1.8663594470046083e-05, 'epoch': 3.48}\n",
      "{'loss': 0.026, 'learning_rate': 1.8433179723502307e-05, 'epoch': 3.5}\n",
      "{'loss': 0.0286, 'learning_rate': 1.8202764976958524e-05, 'epoch': 3.52}\n",
      "{'loss': 0.0301, 'learning_rate': 1.7972350230414748e-05, 'epoch': 3.54}\n",
      "{'loss': 0.0325, 'learning_rate': 1.774193548387097e-05, 'epoch': 3.56}\n",
      "{'loss': 0.0293, 'learning_rate': 1.751152073732719e-05, 'epoch': 3.58}\n",
      "{'loss': 0.032, 'learning_rate': 1.728110599078341e-05, 'epoch': 3.6}\n",
      "{'loss': 0.0307, 'learning_rate': 1.7050691244239634e-05, 'epoch': 3.61}\n",
      "{'loss': 0.0342, 'learning_rate': 1.682027649769585e-05, 'epoch': 3.63}\n",
      "{'loss': 0.0335, 'learning_rate': 1.6589861751152075e-05, 'epoch': 3.65}\n",
      "{'loss': 0.0311, 'learning_rate': 1.6359447004608296e-05, 'epoch': 3.67}\n",
      "{'loss': 0.0357, 'learning_rate': 1.6129032258064517e-05, 'epoch': 3.69}\n",
      "{'loss': 0.036, 'learning_rate': 1.5898617511520737e-05, 'epoch': 3.71}\n",
      "{'loss': 0.0275, 'learning_rate': 1.5668202764976958e-05, 'epoch': 3.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../resources/output/SST2Processed2-train\\checkpoint-2000\n",
      "Configuration saved in ../resources/output/SST2Processed2-train\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0259, 'learning_rate': 1.543778801843318e-05, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../resources/output/SST2Processed2-train\\checkpoint-2000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0281, 'learning_rate': 1.5207373271889403e-05, 'epoch': 3.76}\n",
      "{'loss': 0.0312, 'learning_rate': 1.4976958525345622e-05, 'epoch': 3.78}\n",
      "{'loss': 0.0269, 'learning_rate': 1.4746543778801844e-05, 'epoch': 3.8}\n",
      "{'loss': 0.0279, 'learning_rate': 1.4516129032258066e-05, 'epoch': 3.82}\n",
      "{'loss': 0.0321, 'learning_rate': 1.4285714285714285e-05, 'epoch': 3.84}\n",
      "{'loss': 0.0333, 'learning_rate': 1.4055299539170508e-05, 'epoch': 3.86}\n",
      "{'loss': 0.0386, 'learning_rate': 1.382488479262673e-05, 'epoch': 3.88}\n",
      "{'loss': 0.0345, 'learning_rate': 1.3594470046082949e-05, 'epoch': 3.9}\n",
      "{'loss': 0.0303, 'learning_rate': 1.3364055299539171e-05, 'epoch': 3.91}\n",
      "{'loss': 0.0273, 'learning_rate': 1.313364055299539e-05, 'epoch': 3.93}\n",
      "{'loss': 0.0359, 'learning_rate': 1.2903225806451613e-05, 'epoch': 3.95}\n",
      "{'loss': 0.0242, 'learning_rate': 1.2672811059907835e-05, 'epoch': 3.97}\n",
      "{'loss': 0.029, 'learning_rate': 1.2442396313364056e-05, 'epoch': 3.99}\n",
      "{'loss': 0.0227, 'learning_rate': 1.2211981566820276e-05, 'epoch': 4.01}\n",
      "{'loss': 0.028, 'learning_rate': 1.1981566820276497e-05, 'epoch': 4.03}\n",
      "{'loss': 0.0215, 'learning_rate': 1.175115207373272e-05, 'epoch': 4.04}\n",
      "{'loss': 0.0235, 'learning_rate': 1.152073732718894e-05, 'epoch': 4.06}\n",
      "{'loss': 0.0207, 'learning_rate': 1.129032258064516e-05, 'epoch': 4.08}\n",
      "{'loss': 0.0222, 'learning_rate': 1.1059907834101383e-05, 'epoch': 4.1}\n",
      "{'loss': 0.0219, 'learning_rate': 1.0829493087557604e-05, 'epoch': 4.12}\n",
      "{'loss': 0.0252, 'learning_rate': 1.0599078341013826e-05, 'epoch': 4.14}\n",
      "{'loss': 0.0188, 'learning_rate': 1.0368663594470047e-05, 'epoch': 4.16}\n",
      "{'loss': 0.0225, 'learning_rate': 1.0138248847926269e-05, 'epoch': 4.18}\n",
      "{'loss': 0.0196, 'learning_rate': 9.90783410138249e-06, 'epoch': 4.19}\n",
      "{'loss': 0.0266, 'learning_rate': 9.67741935483871e-06, 'epoch': 4.21}\n",
      "{'loss': 0.0188, 'learning_rate': 9.447004608294931e-06, 'epoch': 4.23}\n",
      "{'loss': 0.0241, 'learning_rate': 9.216589861751153e-06, 'epoch': 4.25}\n",
      "{'loss': 0.0203, 'learning_rate': 8.986175115207374e-06, 'epoch': 4.27}\n",
      "{'loss': 0.0217, 'learning_rate': 8.755760368663595e-06, 'epoch': 4.29}\n",
      "{'loss': 0.0234, 'learning_rate': 8.525345622119817e-06, 'epoch': 4.31}\n",
      "{'loss': 0.0191, 'learning_rate': 8.294930875576038e-06, 'epoch': 4.33}\n",
      "{'loss': 0.0218, 'learning_rate': 8.064516129032258e-06, 'epoch': 4.34}\n",
      "{'loss': 0.0203, 'learning_rate': 7.834101382488479e-06, 'epoch': 4.36}\n",
      "{'loss': 0.0233, 'learning_rate': 7.603686635944701e-06, 'epoch': 4.38}\n",
      "{'loss': 0.0251, 'learning_rate': 7.373271889400922e-06, 'epoch': 4.4}\n",
      "{'loss': 0.0227, 'learning_rate': 7.142857142857143e-06, 'epoch': 4.42}\n",
      "{'loss': 0.0254, 'learning_rate': 6.912442396313365e-06, 'epoch': 4.44}\n",
      "{'loss': 0.021, 'learning_rate': 6.682027649769586e-06, 'epoch': 4.46}\n",
      "{'loss': 0.0239, 'learning_rate': 6.451612903225806e-06, 'epoch': 4.48}\n",
      "{'loss': 0.0174, 'learning_rate': 6.221198156682028e-06, 'epoch': 4.49}\n",
      "{'loss': 0.0204, 'learning_rate': 5.9907834101382485e-06, 'epoch': 4.51}\n",
      "{'loss': 0.0247, 'learning_rate': 5.76036866359447e-06, 'epoch': 4.53}\n",
      "{'loss': 0.022, 'learning_rate': 5.5299539170506915e-06, 'epoch': 4.55}\n",
      "{'loss': 0.0251, 'learning_rate': 5.299539170506913e-06, 'epoch': 4.57}\n",
      "{'loss': 0.0225, 'learning_rate': 5.0691244239631346e-06, 'epoch': 4.59}\n",
      "{'loss': 0.0227, 'learning_rate': 4.838709677419355e-06, 'epoch': 4.61}\n",
      "{'loss': 0.0234, 'learning_rate': 4.608294930875577e-06, 'epoch': 4.63}\n",
      "{'loss': 0.0269, 'learning_rate': 4.377880184331797e-06, 'epoch': 4.64}\n",
      "{'loss': 0.0239, 'learning_rate': 4.147465437788019e-06, 'epoch': 4.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../resources/output/SST2Processed2-train\\checkpoint-2500\n",
      "Configuration saved in ../resources/output/SST2Processed2-train\\checkpoint-2500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.024, 'learning_rate': 3.9170506912442395e-06, 'epoch': 4.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../resources/output/SST2Processed2-train\\checkpoint-2500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0219, 'learning_rate': 3.686635944700461e-06, 'epoch': 4.7}\n",
      "{'loss': 0.0196, 'learning_rate': 3.4562211981566825e-06, 'epoch': 4.72}\n",
      "{'loss': 0.0182, 'learning_rate': 3.225806451612903e-06, 'epoch': 4.74}\n",
      "{'loss': 0.0215, 'learning_rate': 2.9953917050691243e-06, 'epoch': 4.76}\n",
      "{'loss': 0.0226, 'learning_rate': 2.7649769585253458e-06, 'epoch': 4.78}\n",
      "{'loss': 0.0197, 'learning_rate': 2.5345622119815673e-06, 'epoch': 4.79}\n",
      "{'loss': 0.0238, 'learning_rate': 2.3041474654377884e-06, 'epoch': 4.81}\n",
      "{'loss': 0.0242, 'learning_rate': 2.0737327188940094e-06, 'epoch': 4.83}\n",
      "{'loss': 0.0298, 'learning_rate': 1.8433179723502305e-06, 'epoch': 4.85}\n",
      "{'loss': 0.0251, 'learning_rate': 1.6129032258064516e-06, 'epoch': 4.87}\n",
      "{'loss': 0.0227, 'learning_rate': 1.3824884792626729e-06, 'epoch': 4.89}\n",
      "{'loss': 0.0231, 'learning_rate': 1.1520737327188942e-06, 'epoch': 4.91}\n",
      "{'loss': 0.0171, 'learning_rate': 9.216589861751153e-07, 'epoch': 4.93}\n",
      "{'loss': 0.0223, 'learning_rate': 6.912442396313364e-07, 'epoch': 4.94}\n",
      "{'loss': 0.021, 'learning_rate': 4.6082949308755763e-07, 'epoch': 4.96}\n",
      "{'loss': 0.0222, 'learning_rate': 2.3041474654377881e-07, 'epoch': 4.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in ../resources/output/SST2Processed2-train/final\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0244, 'learning_rate': 0.0, 'epoch': 5.0}\n",
      "{'train_runtime': 35630.9558, 'train_samples_per_second': 1.199, 'train_steps_per_second': 0.075, 'train_loss': 0.04242740888497356, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../resources/output/SST2Processed2-train/final\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model = train(dataset, n_classes, embeddings_df, epochs=5, output_dir=output_dir, logging_dir=logging_dir)\n",
    "model.save_pretrained(output_dir + \"/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 13176\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2472\n",
      "  0%|          | 10/2472 [04:38<19:25:58, 28.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1321, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/2472 [09:16<18:55:01, 27.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1136, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 30/2472 [13:47<18:23:53, 27.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0396, 'learning_rate': 3e-06, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 40/2472 [18:19<18:27:21, 27.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9914, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 50/2472 [22:54<18:17:21, 27.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9461, 'learning_rate': 5e-06, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 60/2472 [27:25<18:10:49, 27.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8986, 'learning_rate': 6e-06, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 70/2472 [31:56<18:01:29, 27.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8014, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 80/2472 [36:31<18:10:39, 27.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9407, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 90/2472 [41:04<18:20:51, 27.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7799, 'learning_rate': 9e-06, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 100/2472 [45:37<17:59:16, 27.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7643, 'learning_rate': 1e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 110/2472 [50:13<18:13:46, 27.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.77, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 120/2472 [54:46<17:47:20, 27.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7709, 'learning_rate': 1.2e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 130/2472 [59:19<17:41:21, 27.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7298, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 140/2472 [1:03:51<17:35:12, 27.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7445, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 150/2472 [1:08:22<17:26:49, 27.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6836, 'learning_rate': 1.5e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 160/2472 [1:12:55<17:21:10, 27.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8125, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 170/2472 [1:17:27<17:25:16, 27.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.771, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 180/2472 [1:21:58<17:10:23, 26.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7202, 'learning_rate': 1.8e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 190/2472 [1:26:28<17:10:00, 27.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8799, 'learning_rate': 1.9e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 200/2472 [1:31:00<17:04:51, 27.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7162, 'learning_rate': 2e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 210/2472 [1:35:30<17:01:52, 27.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7203, 'learning_rate': 2.1e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 220/2472 [1:40:01<16:57:37, 27.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7458, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 230/2472 [1:44:33<16:55:43, 27.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8245, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 240/2472 [1:49:06<16:58:06, 27.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7228, 'learning_rate': 2.4e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 250/2472 [1:53:39<16:53:09, 27.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6751, 'learning_rate': 2.5e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 260/2472 [1:58:15<16:48:18, 27.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7099, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 270/2472 [2:02:44<16:25:24, 26.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6674, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 280/2472 [2:07:14<16:24:48, 26.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8146, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 290/2472 [2:11:44<16:34:27, 27.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6554, 'learning_rate': 2.9e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 300/2472 [2:16:22<16:48:26, 27.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6093, 'learning_rate': 3e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 310/2472 [2:20:55<16:15:57, 27.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6759, 'learning_rate': 3.1e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 320/2472 [2:25:25<16:03:36, 26.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7037, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 330/2472 [2:29:55<16:01:33, 26.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8191, 'learning_rate': 3.3e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 340/2472 [2:34:24<15:55:35, 26.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6998, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 350/2472 [2:38:53<16:00:15, 27.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7815, 'learning_rate': 3.5e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 360/2472 [2:43:22<15:42:58, 26.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7191, 'learning_rate': 3.6e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 370/2472 [2:47:52<15:42:31, 26.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7509, 'learning_rate': 3.7e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 380/2472 [2:52:24<15:49:06, 27.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6984, 'learning_rate': 3.8e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 390/2472 [2:56:55<15:35:41, 26.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8204, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 400/2472 [3:01:23<15:25:15, 26.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7405, 'learning_rate': 4e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 410/2472 [3:05:51<15:17:08, 26.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7959, 'learning_rate': 4.1e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 420/2472 [3:10:19<15:18:52, 26.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6365, 'learning_rate': 4.2e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 430/2472 [3:14:49<15:15:26, 26.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7625, 'learning_rate': 4.3e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 440/2472 [3:19:16<14:59:34, 26.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7008, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 450/2472 [3:23:43<15:00:53, 26.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6058, 'learning_rate': 4.5e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 460/2472 [3:28:11<15:00:19, 26.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7783, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 470/2472 [3:32:40<14:56:02, 26.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7027, 'learning_rate': 4.7e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 480/2472 [3:37:09<14:56:19, 27.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6234, 'learning_rate': 4.8e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 490/2472 [3:41:40<14:56:23, 27.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7099, 'learning_rate': 4.9e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 500/2472 [3:46:09<14:39:47, 26.77s/it]Saving model checkpoint to ../resources/output/TweetsProcessed_Fold2\\checkpoint-500\n",
      "Configuration saved in ../resources/output/TweetsProcessed_Fold2\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5821, 'learning_rate': 5e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../resources/output/TweetsProcessed_Fold2\\checkpoint-500\\pytorch_model.bin\n",
      " 21%|██        | 510/2472 [3:50:41<14:40:16, 26.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6887, 'learning_rate': 4.974645030425964e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 520/2472 [3:55:11<14:39:19, 27.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6864, 'learning_rate': 4.949290060851927e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 530/2472 [3:59:41<14:28:12, 26.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7861, 'learning_rate': 4.923935091277891e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 540/2472 [4:04:10<14:25:48, 26.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7535, 'learning_rate': 4.898580121703854e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 550/2472 [4:08:43<14:36:12, 27.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8945, 'learning_rate': 4.873225152129818e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 560/2472 [4:13:13<14:18:33, 26.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6996, 'learning_rate': 4.847870182555781e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 570/2472 [4:17:41<14:08:11, 26.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.672, 'learning_rate': 4.8225152129817444e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 580/2472 [4:22:13<14:23:15, 27.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7025, 'learning_rate': 4.7971602434077076e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 590/2472 [4:26:42<14:05:02, 26.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7745, 'learning_rate': 4.7718052738336714e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 600/2472 [4:31:10<13:55:03, 26.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6667, 'learning_rate': 4.746450304259635e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 610/2472 [4:35:37<13:42:37, 26.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6978, 'learning_rate': 4.7210953346855984e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 620/2472 [4:40:04<13:45:49, 26.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7471, 'learning_rate': 4.695740365111562e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 630/2472 [4:44:32<13:42:21, 26.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6332, 'learning_rate': 4.6703853955375254e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 640/2472 [4:49:02<13:49:19, 27.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.663, 'learning_rate': 4.645030425963489e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 650/2472 [4:53:32<13:35:25, 26.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7098, 'learning_rate': 4.6196754563894524e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 660/2472 [4:58:02<13:32:05, 26.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6688, 'learning_rate': 4.594320486815416e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 670/2472 [5:02:29<13:17:15, 26.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6841, 'learning_rate': 4.5689655172413794e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 680/2472 [5:06:56<13:21:29, 26.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.746, 'learning_rate': 4.543610547667343e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 690/2472 [5:11:26<13:25:17, 27.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.722, 'learning_rate': 4.5182555780933065e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 700/2472 [5:15:55<13:11:28, 26.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7133, 'learning_rate': 4.4929006085192696e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 710/2472 [5:20:25<13:17:12, 27.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6223, 'learning_rate': 4.4675456389452335e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 720/2472 [5:24:54<13:03:36, 26.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9443, 'learning_rate': 4.4421906693711966e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 730/2472 [5:29:22<12:53:16, 26.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7526, 'learning_rate': 4.4168356997971605e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 740/2472 [5:33:52<12:58:33, 26.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6993, 'learning_rate': 4.3914807302231236e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 750/2472 [5:38:20<12:47:22, 26.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7198, 'learning_rate': 4.3661257606490875e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 760/2472 [5:42:50<12:49:29, 26.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5971, 'learning_rate': 4.340770791075051e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 770/2472 [5:47:16<12:35:51, 26.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6404, 'learning_rate': 4.3154158215010145e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 780/2472 [5:51:42<12:31:34, 26.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6117, 'learning_rate': 4.290060851926978e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 790/2472 [5:56:10<12:34:04, 26.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7753, 'learning_rate': 4.2647058823529415e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 800/2472 [6:00:37<12:18:14, 26.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7054, 'learning_rate': 4.2393509127789046e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 810/2472 [6:05:04<12:18:30, 26.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7028, 'learning_rate': 4.213995943204868e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 812/2472 [6:05:58<12:23:25, 26.87s/it]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'nan' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\T-Gamer\\Documents\\SideDrive\\UFMA\\2022.1\\Topicos Especiais (NLP)\\Exercicios\\Trabalho Final\\Implementação\\source\\exp3_transformers.ipynb Célula: 6\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         os\u001b[39m.\u001b[39mmakedirs(\u001b[39mdir\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m dataset \u001b[39m=\u001b[39m Dataset(dataset_df, embeddings_df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mtolist(), label_column\u001b[39m=\u001b[39mlabel_column, n_classes\u001b[39m=\u001b[39mn_classes)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m model \u001b[39m=\u001b[39m train(dataset, embeddings_df, epochs\u001b[39m=\u001b[39;49mn_epochs, output_dir\u001b[39m=\u001b[39;49moutput_dir, logging_dir\u001b[39m=\u001b[39;49mlogging_dir)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(output_dir \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/final\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\T-Gamer\\Documents\\SideDrive\\UFMA\\2022.1\\Topicos Especiais (NLP)\\Exercicios\\Trabalho Final\\Implementação\\source\\exp3_transformers.ipynb Célula: 6\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, embeddings, epochs, output_dir, logging_dir)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     output_dir\u001b[39m=\u001b[39moutput_dir,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39mepochs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     logging_steps\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mdataset\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1498\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1495\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1497\u001b[0m )\n\u001b[1;32m-> 1498\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1499\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1500\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1501\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1502\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1503\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1714\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[0;32m   1713\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1714\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1715\u001b[0m \n\u001b[0;32m   1716\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[0;32m   1717\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1718\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\T-Gamer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\T-Gamer\\Documents\\SideDrive\\UFMA\\2022.1\\Topicos Especiais (NLP)\\Exercicios\\Trabalho Final\\Implementação\\source\\exp3_transformers.ipynb Célula: 6\u001b[0m in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index) :\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# print(index, type(index))\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     sequence, mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_proxessing(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataframe\u001b[39m.\u001b[39;49miloc[index][\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_column])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataframe\u001b[39m.\u001b[39miloc[index][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_column]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m :\n",
      "\u001b[1;32mc:\\Users\\T-Gamer\\Documents\\SideDrive\\UFMA\\2022.1\\Topicos Especiais (NLP)\\Exercicios\\Trabalho Final\\Implementação\\source\\exp3_transformers.ipynb Célula: 6\u001b[0m in \u001b[0;36mDataset.text_proxessing\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m sequence \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokenlist :\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# if not token in self.vocabulario :\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m#     voc_sort = self.vocabulario.copy()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m#     voc_sort.sort(key=lambda x : distance(token, x))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m#     token = voc_sort[0]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     sequence\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocabulario\u001b[39m.\u001b[39;49mindex(token) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m mask \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(sequence))]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/T-Gamer/Documents/SideDrive/UFMA/2022.1/Topicos%20Especiais%20%28NLP%29/Exercicios/Trabalho%20Final/Implementa%C3%A7%C3%A3o/source/exp3_transformers.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(sequence) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length :\n",
      "\u001b[1;31mValueError\u001b[0m: 'nan' is not in list"
     ]
    }
   ],
   "source": [
    "# Para dataset com k-folding\n",
    "# Args\n",
    "k = 10\n",
    "dataset_folder = \"../resources/datasets/TwitterAirlines\"\n",
    "dataset_name   = \"TweetsProcessed\"\n",
    "n_classes = 3\n",
    "n_epochs = 3\n",
    "label_column = \"class\"\n",
    "# Exec\n",
    "folds = []\n",
    "for i_fold in range(k) :\n",
    "    dataset_fold_name = f\"{dataset_name}_Fold{i_fold + 1}\"\n",
    "    fold_df = pd.read_csv(f\"{dataset_folder}/folds/{dataset_fold_name}.csv\", index_col = 0)\n",
    "    folds.append(fold_df)\n",
    "for i_fold in range(1, k) :\n",
    "    dataset_fold_name = f\"{dataset_name}_Fold{i_fold + 1}\"\n",
    "    embeddings_df = pd.read_csv(f\"{dataset_folder}/embeddings/{dataset_fold_name}.csv\", index_col=0)\n",
    "    dataset_df    = pd.concat(folds[ : i_fold] + folds[i_fold + 1 : ])\n",
    "    \n",
    "    output_dir    = f\"../resources/output/{dataset_fold_name}\"\n",
    "    logging_dir   = f\"../resources/logs/{dataset_fold_name}\"\n",
    "    for dir in (output_dir, logging_dir) :\n",
    "        if not os.path.exists(dir) : \n",
    "            os.makedirs(dir)\n",
    "\n",
    "    dataset = Dataset(dataset_df, embeddings_df.index.tolist(), label_column=label_column, n_classes=n_classes)\n",
    "    model = train(dataset, embeddings_df, epochs=n_epochs, output_dir=output_dir, logging_dir=logging_dir)\n",
    "    model.save_pretrained(output_dir + \"/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb071969cc081d156db4658a52cc12ff3302089485c0c8cb524fcf02c6362775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
